{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Test of Oseti against other SA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import oseti\n",
    "import statistics\n",
    "import nltk\n",
    "#nltk.download('all') #runs first time only\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here, the Oseti library is used in a slightly modified form. Oseti is dependent on the MeCab tokenization ,however, the current version of Oseti was not adjusted for the updated MeCab. The code may be adjusted for MeCab, but in this notebook we use Neolog Dictionary with Janome as a tokenizer. In practice, this has almost no effect for the sentiment score. In the analysis of the corpora, pure Oseti was used with minor adjustments for compatibility with a newer MeCab version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "analyzer = oseti.Analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oseti-dictionary based sentiment analysis vs rule-based VADER\n",
    "Here, we juxtapose the two approaches. Oseti sentiment analyzer has a built-in sentence tokenizer, while VADER demands usage of a particular tokenizers (like one in the NLTK package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"text samplings\\\\direct speech sampling JA.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "sampling_ja = text.split(\"\\n\")\n",
    "\n",
    "with open (\"text samplings\\\\direct speech sampling EN.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "sampling_en = text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>私は狼狼した。</td>\n",
       "      <td>I was in a panic.</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>声は細かったが精一杯の喜びの声だ。</td>\n",
       "      <td>My voice was thin, but it was the best voice I...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>硬直せぬ前に婦長さんが体を整える。</td>\n",
       "      <td>Before I could stiffen, the head nurse adjuste...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>家への強烈な不安が、 またも頭にのしかかって来たが、たちまち忙しさにきりきり舞いを始めて、そ...</td>\n",
       "      <td>A strong sense of anxiety about home once agai...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>家を出るとき輝一が冗談ともっかずに云うと、  道子は肩を叩いて送り出した。</td>\n",
       "      <td>When Teruichi said this jokingly as he left th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Original  \\\n",
       "0                                            私は狼狼した。   \n",
       "1                                  声は細かったが精一杯の喜びの声だ。   \n",
       "2                                  硬直せぬ前に婦長さんが体を整える。   \n",
       "3  家への強烈な不安が、 またも頭にのしかかって来たが、たちまち忙しさにきりきり舞いを始めて、そ...   \n",
       "4              家を出るとき輝一が冗談ともっかずに云うと、  道子は肩を叩いて送り出した。   \n",
       "\n",
       "                                         Translation  Sentiment  Unnamed: 3  \\\n",
       "0                                  I was in a panic.         -1         NaN   \n",
       "1  My voice was thin, but it was the best voice I...          0         NaN   \n",
       "2  Before I could stiffen, the head nurse adjuste...         -1         NaN   \n",
       "3  A strong sense of anxiety about home once agai...         -1         NaN   \n",
       "4  When Teruichi said this jokingly as he left th...          0         NaN   \n",
       "\n",
       "   Unnamed: 4 Unnamed: 5  \n",
       "0         NaN        NaN  \n",
       "1         NaN        NaN  \n",
       "2         NaN        NaN  \n",
       "3         NaN             \n",
       "4         NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_dataset = pd.read_excel(\"text samplings\\\\annotated dataset.xlsx\", index_col=False)\n",
    "annotated_sents_ja = list(annotated_dataset[\"Original\"])\n",
    "annotated_sents_en = list(annotated_dataset[\"Translation\"])\n",
    "true_annotated_scores = list(annotated_dataset[\"Sentiment\"])\n",
    "annotated_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of samplings\n",
    "no_sampling_ja = len(sampling_ja)\n",
    "no_sampling_en = len(sampling_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oseti results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "oseti_sentiment = [statistics.mean(analyzer.analyze(sent)) for sent in sampling_ja]\n",
    "oseti_annotated = [statistics.mean(analyzer.analyze(sent)) for sent in annotated_sents_ja]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment = [sia.polarity_scores(sent)['compound'] for sent in sampling_en]\n",
    "vader_annotated = [sia.polarity_scores(sent)['compound'] for sent in annotated_sents_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-finetuned-japanese-sentiment\n",
    "Training dataset: Amazon Reviews\\\n",
    "No.: 20000 reviews\\\n",
    "Link: https://huggingface.co/christian-phu/bert-finetuned-japanese-sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "100%|██████████| 159/159 [00:24<00:00,  6.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# model needs the following dependencies:\n",
    "#!pip install fugashi\n",
    "#!pip install unidic_lite\n",
    "\n",
    "\n",
    "sentiment_analyzer = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"christian-phu/bert-finetuned-japanese-sentiment\"\n",
    "        )\n",
    "bert_sentiment = []\n",
    "for sent in tqdm(sampling_ja):\n",
    "    result = sentiment_analyzer(sent)[0]\n",
    "    label_to_score = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    compound_score = label_to_score[result['label']] * result['score']\n",
    "    bert_sentiment.append(compound_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.87it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_annotated = []\n",
    "for sent in tqdm(annotated_sents_ja):\n",
    "    result = sentiment_analyzer(sent)[0]\n",
    "    label_to_score = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    compound_score = label_to_score[result['label']]\n",
    "    bert_annotated.append(compound_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### japanese-sentiment-analysis\n",
    "Training dataset: Corporate financial reports\\\n",
    "No.: 200 reports (6,119 sentences)\\\n",
    "Link: https://huggingface.co/jarvisx17/japanese-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "100%|██████████| 159/159 [00:22<00:00,  7.23it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_jarv = pipeline(\"sentiment-analysis\", model=\"jarvisx17/japanese-sentiment-analysis\")\n",
    "jarv_sentiment = []\n",
    "for sent in tqdm(sampling_ja):\n",
    "    result = sentiment_analyzer_jarv(sent)[0]\n",
    "    label_to_score = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    compound_score = label_to_score[result['label']] * result['score']\n",
    "    jarv_sentiment.append(compound_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 12.09it/s]\n"
     ]
    }
   ],
   "source": [
    "jarv_annotated = []\n",
    "for sent in tqdm(annotated_sents_ja):\n",
    "    result = sentiment_analyzer_jarv(sent)[0]\n",
    "    label_to_score = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    compound_score = label_to_score[result['label']]\n",
    "    jarv_annotated.append(compound_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Japanese Stock Comment Sentiment Model\n",
    "Training dataset: Comments and discussions related to Japanese stocks\\\n",
    "No.: Not clarified\\\n",
    "Link: https://huggingface.co/c299m/japanese_stock_sentiment\\\n",
    "\\\n",
    "\\\n",
    "This model is inapplicable for SA, as it estimates only market trends in two categories: \"bullish\" and \"bearish\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finance-sentiment-ja-base\n",
    "Training dataset: Japanese financial news\\\n",
    "No.: ≈5,000 sentences/phrases\\\n",
    "Link: https://huggingface.co/bardsai/finance-sentiment-ja-base\\\n",
    "\\\n",
    "The model is unoperabable as in the majority of cases it outputs neutral sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "100%|██████████| 159/159 [00:17<00:00,  8.89it/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer_bardsai = pipeline(\"sentiment-analysis\", model=\"bardsai/finance-sentiment-ja-base\")\n",
    "bardsai_sentiment = []\n",
    "for sent in tqdm(sampling_ja):\n",
    "    result = sentiment_analyzer_bardsai(sent)[0]\n",
    "    label_to_score = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    compound_score = label_to_score[result['label']] * result['score']\n",
    "    bardsai_sentiment.append(compound_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:11<00:00,  9.01it/s]\n"
     ]
    }
   ],
   "source": [
    "bardsai_annotated = []\n",
    "for sent in tqdm(annotated_sents_ja):\n",
    "    result = sentiment_analyzer_bardsai(sent)[0]\n",
    "    label_to_score = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "    compound_score = label_to_score[result['label']]\n",
    "    bardsai_annotated.append(compound_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparative_df = pd.DataFrame({\"Oseti\": oseti_sentiment, \"VADER\": vader_sentiment, \"bert-finetuned-japanese-sentiment\": bert_sentiment,\n",
    "                               \"japanese-sentiment-analysis\": jarv_sentiment,\"finance-sentiment-ja-base\": bardsai_sentiment})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparative_df.to_csv(\"Models Overview Dataframe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Oseti</th>\n",
       "      <th>VADER</th>\n",
       "      <th>bert-finetuned-japanese-sentiment</th>\n",
       "      <th>japanese-sentiment-analysis</th>\n",
       "      <th>finance-sentiment-ja-base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>159.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.058192</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.374328</td>\n",
       "      <td>0.314734</td>\n",
       "      <td>0.010143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.517422</td>\n",
       "      <td>0.384876</td>\n",
       "      <td>0.646258</td>\n",
       "      <td>0.923552</td>\n",
       "      <td>0.145701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.928700</td>\n",
       "      <td>-0.997148</td>\n",
       "      <td>-0.999939</td>\n",
       "      <td>-0.999920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.064400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.988994</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652621</td>\n",
       "      <td>0.988310</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.984478</td>\n",
       "      <td>0.999605</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900100</td>\n",
       "      <td>0.999272</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>0.999010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Oseti       VADER  bert-finetuned-japanese-sentiment  \\\n",
       "count  159.000000  159.000000                         159.000000   \n",
       "mean     0.058192    0.002313                           0.374328   \n",
       "std      0.517422    0.384876                           0.646258   \n",
       "min     -1.000000   -0.928700                          -0.997148   \n",
       "25%      0.000000   -0.064400                           0.000000   \n",
       "50%      0.000000    0.000000                           0.652621   \n",
       "75%      0.000000    0.137800                           0.984478   \n",
       "max      1.000000    0.900100                           0.999272   \n",
       "\n",
       "       japanese-sentiment-analysis  finance-sentiment-ja-base  \n",
       "count                   159.000000                 159.000000  \n",
       "mean                      0.314734                   0.010143  \n",
       "std                       0.923552                   0.145701  \n",
       "min                      -0.999939                  -0.999920  \n",
       "25%                      -0.988994                   0.000000  \n",
       "50%                       0.988310                   0.000000  \n",
       "75%                       0.999605                   0.000000  \n",
       "max                       0.999955                   0.999010  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparative_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer models for Japanese sentiment analysis did not demonstrate a strong rationale for their advantage over the simplistic, dictionary-based method used by Oseti.\n",
    "\n",
    "1. They do not provide a direct interface for calculating sentiment intensity. Instead, intensity scores are indirectly inferred from the model’s confidence (probability) in classifying a sentence as positive, negative, or neutral.\n",
    "\n",
    "2. Among the four documented models, only two are operational. The Japanese Stock Comment Sentiment Model is not suitable for this study, as its sentiment classes (\"bearish\" and \"bullish\") do not align with the required categories. The finance-sentiment-ja-base model tends to classify most sentences as neutral when applied to samples from the Atomic Bomb Literature corpus.\n",
    "\n",
    "3. The transformer models did not demonstrate a meaningfully stronger correlation with the VADER model, nor among themselves.\n",
    "\n",
    "4. Given the advantages of rule-based models like VADER—particularly their transparency and traceability—we consider VADER a reliable reference point. When comparing against this benchmark, Oseti shows significantly better alignment. Although finance-sentiment-ja-base produced slightly higher precision, recall, and F1 scores, 155 out of its 157 predictions were classified as neutral, limiting its practical usefulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Oseti</th>\n",
       "      <th>VADER</th>\n",
       "      <th>bert-finetuned-japanese-sentiment</th>\n",
       "      <th>japanese-sentiment-analysis</th>\n",
       "      <th>finance-sentiment-ja-base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Oseti</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.357845</td>\n",
       "      <td>0.268596</td>\n",
       "      <td>0.271637</td>\n",
       "      <td>0.155789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VADER</th>\n",
       "      <td>0.357845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.345126</td>\n",
       "      <td>0.381054</td>\n",
       "      <td>0.153501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bert-finetuned-japanese-sentiment</th>\n",
       "      <td>0.268596</td>\n",
       "      <td>0.345126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.329962</td>\n",
       "      <td>0.000454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japanese-sentiment-analysis</th>\n",
       "      <td>0.271637</td>\n",
       "      <td>0.381054</td>\n",
       "      <td>0.329962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.146033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finance-sentiment-ja-base</th>\n",
       "      <td>0.155789</td>\n",
       "      <td>0.153501</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.146033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Oseti     VADER  \\\n",
       "Oseti                              1.000000  0.357845   \n",
       "VADER                              0.357845  1.000000   \n",
       "bert-finetuned-japanese-sentiment  0.268596  0.345126   \n",
       "japanese-sentiment-analysis        0.271637  0.381054   \n",
       "finance-sentiment-ja-base          0.155789  0.153501   \n",
       "\n",
       "                                   bert-finetuned-japanese-sentiment  \\\n",
       "Oseti                                                       0.268596   \n",
       "VADER                                                       0.345126   \n",
       "bert-finetuned-japanese-sentiment                           1.000000   \n",
       "japanese-sentiment-analysis                                 0.329962   \n",
       "finance-sentiment-ja-base                                   0.000454   \n",
       "\n",
       "                                   japanese-sentiment-analysis  \\\n",
       "Oseti                                                 0.271637   \n",
       "VADER                                                 0.381054   \n",
       "bert-finetuned-japanese-sentiment                     0.329962   \n",
       "japanese-sentiment-analysis                           1.000000   \n",
       "finance-sentiment-ja-base                             0.146033   \n",
       "\n",
       "                                   finance-sentiment-ja-base  \n",
       "Oseti                                               0.155789  \n",
       "VADER                                               0.153501  \n",
       "bert-finetuned-japanese-sentiment                   0.000454  \n",
       "japanese-sentiment-analysis                         0.146033  \n",
       "finance-sentiment-ja-base                           1.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparative_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Oseti</th>\n",
       "      <th>bert-finetuned-japanese-sentiment</th>\n",
       "      <th>japanese-sentiment-analysis</th>\n",
       "      <th>finance-sentiment-ja-base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.584939</td>\n",
       "      <td>0.360627</td>\n",
       "      <td>0.230748</td>\n",
       "      <td>0.657199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.591195</td>\n",
       "      <td>0.345912</td>\n",
       "      <td>0.408805</td>\n",
       "      <td>0.452830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.577118</td>\n",
       "      <td>0.328412</td>\n",
       "      <td>0.289929</td>\n",
       "      <td>0.305955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Oseti  bert-finetuned-japanese-sentiment  \\\n",
       "0  Precision  0.584939                           0.360627   \n",
       "1     Recall  0.591195                           0.345912   \n",
       "2         F1  0.577118                           0.328412   \n",
       "\n",
       "   japanese-sentiment-analysis  finance-sentiment-ja-base  \n",
       "0                     0.230748                   0.657199  \n",
       "1                     0.408805                   0.452830  \n",
       "2                     0.289929                   0.305955  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def transform_sentiment(input_scores):\n",
    "    \"\"\"sent > 0 -> 1; sent < 0 -> -1; sent = 0 -> 0\"\"\"\n",
    "    transformed_sentiment = [1 if score > 0 else (-1 if score < 0 else 0) for score in input_scores]\n",
    "    return transformed_sentiment\n",
    "\n",
    "y_vader = transform_sentiment(vader_sentiment)  \n",
    "y_oseti = transform_sentiment(oseti_sentiment)\n",
    "y_bert = transform_sentiment(bert_sentiment)\n",
    "y_jarv = transform_sentiment(jarv_sentiment)\n",
    "y_bardsai = transform_sentiment(bardsai_sentiment)\n",
    "\n",
    "def get_metrics(true_values, predicted_values):\n",
    "    \"\"\"Calculates precision, recall, and F1 score\"\"\"\n",
    "    precision = precision_score(true_values, predicted_values, average='weighted')  # Using 'weighted' for multi-class\n",
    "    recall = recall_score(true_values, predicted_values, average='weighted')\n",
    "    f1 = f1_score(true_values, predicted_values, average='weighted')\n",
    "    return [precision, recall, f1]\n",
    "\n",
    "\n",
    "oseti_metrics = get_metrics(y_vader, y_oseti)\n",
    "bert_metrics = get_metrics(y_vader, y_bert)\n",
    "jarv_metrics = get_metrics(y_vader, y_jarv)\n",
    "bardsai_metrics = get_metrics(y_vader, y_bardsai)\n",
    "\n",
    "metrics_df = pd.DataFrame({\"Metric\": [\"Precision\", \"Recall\", \"F1\"], \"Oseti\": oseti_metrics,\n",
    "        \"bert-finetuned-japanese-sentiment\": bert_metrics, \"japanese-sentiment-analysis\": jarv_metrics,\n",
    "        \"finance-sentiment-ja-base\": bardsai_metrics})\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"Models against VADER tests.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average sentiment  scores for different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058192323050813614\n",
      "0.002313207547169812\n",
      "0.3743280177971102\n",
      "0.3147344626720596\n",
      "0.010143305520591495\n"
     ]
    }
   ],
   "source": [
    "for group in [oseti_sentiment, vader_sentiment, bert_sentiment, jarv_sentiment, bardsai_sentiment]:\n",
    "    print(statistics.mean(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observable from the sentiment scores data for different models, transfomer models tend to output sentiment to more positive values. Again, the model *Finance-sentiment-ja-base* with peculiar behavior demonstrates closeness to VADER and OSETI outputs by tending to \"neutralize\" its outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n",
      "0.6526210308074951\n",
      "0.9883104562759399\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for group in [oseti_sentiment, vader_sentiment, bert_sentiment, jarv_sentiment, bardsai_sentiment]:\n",
    "    print(statistics.median(group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the sampling is limited, the median values demonstrate that the operationable transformers models still tend to make the output sentiment scores more positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models with Annotated Dataset\n",
    "From the previously used sample, 50 lines of direct speech (50 phrases or sentences) and 50 lines of authorial narration were selected. I manually annotated these 100 samples with one of three sentiment labels: positive (1), negative (-1), or neutral (0). Although some studies employ human annotation with continuous numeric sentiment values (see, for example, Bizzoni and Feldkamp), I deliberately refrained from this approach.\n",
    "\n",
    "First, such evaluations are highly subjective, and having a limited number of annotators (N = 1) does not provide a sufficient basis for trusting the results. Second, one of the key strengths of sentiment analysis (SA) approaches that output numerical sentiment scores lies in their ability to follow traceable, rule-based processes. This characteristic aligns with the broader goals of computational criticism, which aims to offer new, reproducible perspectives on literary texts (see Ramsay).\n",
    "\n",
    "Therefore, for evaluating the adequacy of model performance, I focus solely on whether the outputs correctly match the general polarity (tonality) of the text. In previous experiments, Transformer-based models showed that they do not output true sentiment scores in a strict sense, but rather reflect their internal confidence in polarity classification. As a result, these scores tend to have limited variation and diverge significantly from the more nuanced distribution of emotion found in natural speech.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>私は狼狼した。</td>\n",
       "      <td>I was in a panic.</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>声は細かったが精一杯の喜びの声だ。</td>\n",
       "      <td>My voice was thin, but it was the best voice I...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>硬直せぬ前に婦長さんが体を整える。</td>\n",
       "      <td>Before I could stiffen, the head nurse adjuste...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>家への強烈な不安が、 またも頭にのしかかって来たが、たちまち忙しさにきりきり舞いを始めて、そ...</td>\n",
       "      <td>A strong sense of anxiety about home once agai...</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>家を出るとき輝一が冗談ともっかずに云うと、  道子は肩を叩いて送り出した。</td>\n",
       "      <td>When Teruichi said this jokingly as he left th...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Original  \\\n",
       "0                                            私は狼狼した。   \n",
       "1                                  声は細かったが精一杯の喜びの声だ。   \n",
       "2                                  硬直せぬ前に婦長さんが体を整える。   \n",
       "3  家への強烈な不安が、 またも頭にのしかかって来たが、たちまち忙しさにきりきり舞いを始めて、そ...   \n",
       "4              家を出るとき輝一が冗談ともっかずに云うと、  道子は肩を叩いて送り出した。   \n",
       "\n",
       "                                         Translation  Sentiment  Unnamed: 3  \\\n",
       "0                                  I was in a panic.         -1         NaN   \n",
       "1  My voice was thin, but it was the best voice I...          0         NaN   \n",
       "2  Before I could stiffen, the head nurse adjuste...         -1         NaN   \n",
       "3  A strong sense of anxiety about home once agai...         -1         NaN   \n",
       "4  When Teruichi said this jokingly as he left th...          0         NaN   \n",
       "\n",
       "   Unnamed: 4 Unnamed: 5  \n",
       "0         NaN        NaN  \n",
       "1         NaN        NaN  \n",
       "2         NaN        NaN  \n",
       "3         NaN             \n",
       "4         NaN        NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentiment(input_scores):\n",
    "    \"\"\"sent > 0 -> 1; sent < 0 -> -1; sent = 0 -> 0\"\"\"\n",
    "    transformed_sentiment = [1 if score > 0 else (-1 if score < 0 else 0) for score in input_scores]\n",
    "    return transformed_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Artem\\AppData\\Local\\Temp\\ipykernel_16708\\412442652.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  expanded_df[\"Oseti\"] = oseti_annotated\n",
      "C:\\Users\\Artem\\AppData\\Local\\Temp\\ipykernel_16708\\412442652.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  expanded_df[\"VADER\"] = transform_sentiment(vader_annotated)\n",
      "C:\\Users\\Artem\\AppData\\Local\\Temp\\ipykernel_16708\\412442652.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  expanded_df[\"bert-finetuned-japanese-sentiment\"] = bert_annotated\n",
      "C:\\Users\\Artem\\AppData\\Local\\Temp\\ipykernel_16708\\412442652.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  expanded_df[\"japanese-sentiment-analysis\"] = jarv_annotated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Oseti</th>\n",
       "      <th>VADER</th>\n",
       "      <th>bert-finetuned-japanese-sentiment</th>\n",
       "      <th>japanese-sentiment-analysis</th>\n",
       "      <th>finance-sentiment-ja-base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment  Oseti  VADER  bert-finetuned-japanese-sentiment  \\\n",
       "0         -1   -1.0     -1                                 -1   \n",
       "1          0    1.0      1                                  1   \n",
       "2         -1    1.0      0                                  0   \n",
       "3         -1   -1.0      1                                  1   \n",
       "4          0    1.0      0                                 -1   \n",
       "\n",
       "   japanese-sentiment-analysis  finance-sentiment-ja-base  \n",
       "0                            1                          0  \n",
       "1                            1                          0  \n",
       "2                            1                          0  \n",
       "3                            1                          0  \n",
       "4                            1                          0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_df = annotated_dataset[[\"Sentiment\"]]\n",
    "expanded_df[\"Oseti\"] = oseti_annotated\n",
    "expanded_df[\"VADER\"] = transform_sentiment(vader_annotated)\n",
    "expanded_df[\"bert-finetuned-japanese-sentiment\"] = bert_annotated\n",
    "expanded_df[\"japanese-sentiment-analysis\"] = jarv_annotated\n",
    "expanded_df[\"finance-sentiment-ja-base\"] = bardsai_annotated\n",
    "expanded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_df.to_csv(\"100 samples annotated dataset with other model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Oseti</th>\n",
       "      <th>VADER</th>\n",
       "      <th>bert-finetuned-japanese-sentiment</th>\n",
       "      <th>japanese-sentiment-analysis</th>\n",
       "      <th>finance-sentiment-ja-base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.577143</td>\n",
       "      <td>0.620889</td>\n",
       "      <td>0.575513</td>\n",
       "      <td>0.150330</td>\n",
       "      <td>0.758763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F1</td>\n",
       "      <td>0.550859</td>\n",
       "      <td>0.574698</td>\n",
       "      <td>0.428713</td>\n",
       "      <td>0.159883</td>\n",
       "      <td>0.419309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric     Oseti     VADER  bert-finetuned-japanese-sentiment  \\\n",
       "0  Precision  0.577143  0.620889                           0.575513   \n",
       "1     Recall  0.560000  0.570000                           0.390000   \n",
       "2         F1  0.550859  0.574698                           0.428713   \n",
       "\n",
       "   japanese-sentiment-analysis  finance-sentiment-ja-base  \n",
       "0                     0.150330                   0.758763  \n",
       "1                     0.220000                   0.550000  \n",
       "2                     0.159883                   0.419309  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "def get_metrics(input_true_values, input_predicted_values):\n",
    "    true_values = [int(value) for value in input_true_values]\n",
    "    predicted_values = [int(value) for value in input_predicted_values]\n",
    "    \"\"\"Calculates precision, recall, and F1 score\"\"\"\n",
    "    precision = precision_score(true_values, predicted_values, average='weighted')  # Using 'weighted' for multi-class\n",
    "    recall = recall_score(true_values, predicted_values, average='weighted')\n",
    "    f1 = f1_score(true_values, predicted_values, average='weighted')\n",
    "    return [precision, recall, f1]\n",
    "\n",
    "\n",
    "oseti_annotated_metrics = get_metrics(true_annotated_scores, expanded_df['Oseti'])\n",
    "vader_annotated_metrics = get_metrics(true_annotated_scores, expanded_df['VADER'])\n",
    "bert_annotated_metrics = get_metrics(true_annotated_scores, expanded_df['bert-finetuned-japanese-sentiment'])\n",
    "jarv_annotated_metrics = get_metrics(true_annotated_scores, expanded_df['japanese-sentiment-analysis'])\n",
    "bardsai_annotated_metrics = get_metrics(true_annotated_scores, expanded_df['finance-sentiment-ja-base'])\n",
    "\n",
    "annotated_metrics_df = pd.DataFrame({\"Metric\": [\"Precision\", \"Recall\", \"F1\"], \"Oseti\": oseti_annotated_metrics,\n",
    "        \"VADER\": vader_annotated_metrics,\n",
    "        \"bert-finetuned-japanese-sentiment\": bert_annotated_metrics, \"japanese-sentiment-analysis\": jarv_annotated_metrics,\n",
    "        \"finance-sentiment-ja-base\": bardsai_annotated_metrics})\n",
    "\n",
    "annotated_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tests with the annotated dataset demonstrate that Oseti outperforms existing Transformer models trained on non-literary texts and limited datasets. In terms of precision, Oseti performs on par with the most suitable model tested—bert-finetuned-japanese-sentiment—but it significantly outperforms it in recall and, consequently, in the F1 score.\n",
    "\n",
    "Meanwhile, when applied to translated texts, the rule-based VADER model demonstrates significantly better precision. In contrast, finance-sentiment-ja-base, which tends to \"neutralize\" sentiment, once again exhibits unreliable behavior.\n",
    "\n",
    "As a result, the minimalistic Oseti model performs competitively with more resource-intensive and complex Transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_metrics_df.to_csv(\"metrics for annotated dataset and other models.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "Bizzoni, Yuri, and Pascale Feldkamp. “Sentiment Analysis for Literary Texts: Hemingway as a Case-Study.” Journal of Data Mining & Digital Humanities, vol. NLP4DH, Apr. 2024. DOI.org (Crossref), https://doi.org/10.46298/jdmdh.13155.\n",
    "\n",
    "Ramsay, Stephen. Reading Machines: Toward an Algorithmic Criticism. University of Illinois Press, 2011.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
