{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Texts Preprossesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter Notebook, we demonstrate basic preprosessing of the corpus of OCRed texts of Atomic Bomb Literature Corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Book source**: 15 Volume *Anthology of Japanese Atomic Bomb Literature* (日本の原爆文学, 東京 : ほるぷ出版 , 1983)\\\n",
    "**Number of Volumes**: 13 (the last two volumes with non-fiction are exluded)\\\n",
    "**Number of works**: 106\n",
    "\\\n",
    "\\\n",
    "The texts were OCR'd using ABBYY FineReader 15. As the program supports validation of the OCR against the raw image and can suggest characters it doubts in recognition, a researcher promptly reviewed each OCR'd page. If incorrect recognition was identified, the researcher manually replaced the misrecognized characters, subjectively estimating about 1 or 2 issues per page, excluding common patterns of misrecognition (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process requires any standard library for operating with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Japanese language uses a specific long space \"　\". Other spaces are not typically present in Japanese, except for those inserted in foreign language text. The misrecognized regular spaces were deleted (only between Japanese characters, to preserve the spaces in foreign language text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_japanese(char):\n",
    "    \"\"\"checks if a character is Japanese.\"\"\"\n",
    "    unicode_point = ord(char)\n",
    "    return (0x3040 <= unicode_point <= 0x309F or\n",
    "            0x30A0 <= unicode_point <= 0x30FF or\n",
    "            0x4E00 <= unicode_point <= 0x9FAF)\n",
    "\n",
    "def clear_extra_spaces (input_text:str):\n",
    "    \"\"\"removes unnecessary common spaces (excluding Japanese spaces) from the text\"\"\"\n",
    "    text = list(input_text)\n",
    "    extra_spaces_index = []\n",
    "\n",
    "    for i in range(1,len(text)-1):\n",
    "        if text[i] == \" \":\n",
    "            if (is_japanese(text[i-1]) and is_japanese(text[i+1])) or (text[i-1] == \" \" or text[i+1] == \" \"):\n",
    "                if (not is_japanese(text[i-1]) and text[i-1] != \" \") and text[i+1] == \" \": \n",
    "                    continue\n",
    "                extra_spaces_index.append(i)\n",
    "        \n",
    "    no_extra_spaces = [text[i] for i in range(len(text)) if i not in extra_spaces_index]\n",
    "    return \"\".join(no_extra_spaces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newline_between_japanese(text):\n",
    "    # Pattern to match a Japanese character, followed by a newline, followed by another Japanese character\n",
    "    pattern = r'([\\p{Script=Hiragana}\\p{Script=Katakana}\\p{Script=Han}])\\n([\\p{Script=Hiragana}\\p{Script=Katakana}\\p{Script=Han}])'\n",
    "    # Replace the pattern with the two Japanese characters without the newline\n",
    "    replaced_text = regex.sub(pattern, r'\\1\\2', text)\n",
    "    return replaced_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reviewing the numerous OCR cases in the corpus, common recognition errors were identified. Manual observation of cases during the OCR process showed that the patterns, listed in the code below, directly correspond to the following characters (mostly incorrect punctuation marks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_ocr_errors(input_text:str): # in this shell the major OCR errors are present\n",
    "    \"\"\"replaces some common ocr errors for the current case\"\"\" \n",
    "    circle = clear_extra_spaces(input_text)\n",
    "    circle = circle.replace(\":::\", \"……\")\n",
    "    circle = circle.replace(\":：:\", \"……\")\n",
    "    circle = circle.replace(\"\t\", \"……\")\n",
    "    circle = circle.replace(\"・て\", \"で\")\n",
    "    circle = circle.replace(\"•て\", \"で\")\n",
    "    circle = circle.replace(\"•\", \"・\")\n",
    "    circle = circle.replace(\":：:〇\", \"……。\")\n",
    "    circle = circle.replace(\"^\", \"。\")\n",
    "    circle = circle.replace(\":：:。\", \"……。\")\n",
    "    circle = circle.replace(\"た〇\", \"た。\")\n",
    "    circle = circle.replace(\"た0\", \"た。\")\n",
    "    circle = remove_newline_between_japanese(circle)\n",
    "    return circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is given as an example and needs the adjustment before running \n",
    "folders = os.listdir(\"texts\")\n",
    "for subfolder in folders:\n",
    "    if not os.path.exists(f\"preprocessed_texts\\\\{subfolder}\"):\n",
    "        os.makedirs(f\"preprocessed_texts\\\\{subfolder}\") \n",
    "\n",
    "    subfolder_files = os.listdir(f\"texts\\\\{subfolder}\")\n",
    "    for doc in subfolder_files:\n",
    "        with open(f\"texts\\\\{subfolder}\\\\{doc}\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "        cleaned_text = correct_ocr_errors(text)\n",
    "        with open(f\"preprocessed_texts\\\\{subfolder}\\\\{doc}\", encoding=\"utf-8\", mode=\"w\") as file:\n",
    "            file.write(cleaned_text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = os. listdir(\"preprocessed_texts\")\n",
    "for folder in folders:\n",
    "    files = os.listdir(f\"preprocessed_texts\\\\{folder}\")\n",
    "    compound_text = \"\"\n",
    "    for doc in files:\n",
    "        with open(f\"preprocessed_texts\\\\{folder}\\\\{doc}\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            compound_text += text\n",
    "    with open(f\"texts per author\\\\{folder}.txt\", encoding=\"utf-8\", mode=\"w\") as file:\n",
    "        file.write(compound_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
